////
////  CameraViewController.m
////
////
////  Created by Colin Power on 2016-04-02.
////  Copyright Â© 2016 Colin Power. All rights reserved.
////
//
//
//#include <opencv2/opencv.hpp>
////using namespace cv;
//#import <opencv2/videoio/cap_ios.h>
//#include "opencv2/imgcodecs.hpp"
//#include "opencv2/imgproc.hpp"
//#include "opencv2/videoio.hpp"
//#include <opencv2/highgui.hpp>
//#include <opencv2/video.hpp>
//
//#import "CameraViewController.h"
//
//@interface CameraViewController () <CvVideoCameraDelegate>
//@property (strong, nonatomic) IBOutlet UIView *VideoVIew;
//
//
//@property (weak, nonatomic) IBOutlet UIImageView *maskView;
//@property (weak, nonatomic) IBOutlet UIButton *startButton;
//- (IBAction)startTapped:(UIButton *)sender;
//@property cv::Mat img, fgmask;
//@property cv::Ptr<cv::BackgroundSubtractor> bg_model;
//@property bool update_bg_model;
//
//// Global variables
//@property cv::Mat frame; //current frame
//@property cv::Mat fgMaskMOG2; //fg mask fg mask generated by MOG2 method
//@property cv::Mat fgMask; //fg mask fg mask generated by MOG2 method
//@property int testCount;
//@property int onesCount;
//@property int onesCountPrevious;
//@property UIImage* videoViewImage;
//
//@property cv::Ptr<cv::BackgroundSubtractor> pMOG2;
//@property (strong, nonatomic)    CvVideoCamera* videoCamera;
////TODO Might be usng this wrong
////@property cv::BackgroundSubtractorMOG2* pMOG2;//MOG2 Background subtractor
//
//
//@end
//
//@implementation CameraViewController
//@synthesize bg_model;
//@synthesize onesCount, onesCountPrevious;
//@synthesize videoViewImage;
//
//- (void)viewDidLoad {
//    [super viewDidLoad];
//    
//    //setup camera stream
//    _videoCamera = [[CvVideoCamera alloc] initWithParentView:_maskView];
//    //[_videoCamera setDefaultAVCaptureSessionPreset:AVCaptureDevicePositionFront];
//    _videoCamera.defaultAVCaptureDevicePosition = AVCaptureDevicePositionBack;
//    //_videoCamera.defaultAVCaptureSessionPreset = AVCaptureSessionPreset352x288;
//    _videoCamera.defaultAVCaptureSessionPreset = AVCaptureSessionPreset1280x720;
//    _videoCamera.defaultAVCaptureVideoOrientation = AVCaptureVideoOrientationPortrait;
//    
//    [_videoCamera lockBalance];
//    [_videoCamera lockExposure];
//    [_videoCamera lockFocus];
//    
//    // _videoCamera.recordVideo = YES;
//    _videoCamera.defaultFPS = 30;
//    _videoCamera.grayscaleMode = NO;
//    _videoCamera.delegate = self;
//    _testCount = 0;
//    
//    //setup MOG
//    /*Ptr<BackgroundSubtractorMOG2> cv::createBackgroundSubtractorMOG2	(	int 	history = 500,
//     double 	varThreshold = 16,
//     bool 	detectShadows = true
//     )*/
//    _pMOG2 = cv::createBackgroundSubtractorMOG2(1500,16, false); //MOG2 approach
//    
//    onesCount = -1;
//    onesCountPrevious = -1;
//}
//
//- (void)didReceiveMemoryWarning {
//    [super didReceiveMemoryWarning];
//    // Dispose of any resources that can be recreated.
//}
//
//#pragma mark - Protocol CvVideoCameraDelegate
//#ifdef __cplusplus
//- (void)processImage:(cv::Mat&)image
//{
//    //process here
//    // cv::cvtColor(image, _img, cv::COLOR_BGRA2RGB);
//    //int fixedWidth = 270;
//    // cv::resize(_img, _img, cv::Size(fixedWidth,(int)((fixedWidth*1.0f)*(image.rows/(image.cols*1.0f)))),cv::INTER_NEAREST);
//    //-(UIImage *)UIImageFromCVMat:(cv::Mat)cvMat
//    
//    /*cv::Mat cvMat = image;
//     {
//     NSData *data = [NSData dataWithBytes:cvMat.data length:cvMat.elemSize()*cvMat.total()];
//     CGColorSpaceRef colorSpace;
//     
//     if (cvMat.elemSize() == 1) {
//     colorSpace = CGColorSpaceCreateDeviceGray();
//     } else {
//     colorSpace = CGColorSpaceCreateDeviceRGB();
//     }
//     
//     CGDataProviderRef provider = CGDataProviderCreateWithCFData((__bridge CFDataRef)data);
//     
//     // Creating CGImage from cv::Mat
//     CGImageRef imageRef = CGImageCreate(cvMat.cols,                                 //width
//     cvMat.rows,                                 //height
//     8,                                          //bits per component
//     8 * cvMat.elemSize(),                       //bits per pixel
//     cvMat.step[0],                            //bytesPerRow
//     colorSpace,                                 //colorspace
//     kCGImageAlphaNone|kCGBitmapByteOrderDefault,// bitmap info
//     provider,                                   //CGDataProviderRef
//     NULL,                                       //decode
//     false,                                      //should interpolate
//     kCGRenderingIntentDefault                   //intent
//     );
//     
//     
//     // Getting UIImage from CGImage
//     videoViewImage = [UIImage imageWithCGImage:imageRef];
//     self.videoView.image = videoViewImage;
//     
//     CGImageRelease(imageRef);
//     CGDataProviderRelease(provider);
//     CGColorSpaceRelease(colorSpace);
//     // return finalImage;
//     }*/
//    
//    
//    _pMOG2->apply(image, _fgMaskMOG2, -1);
//    
//    //process out noise on the mask
//    cv::GaussianBlur(_fgMaskMOG2, _fgMaskMOG2, cv::Size(3, 3), 2, 2);
//    cv::threshold(_fgMaskMOG2, _fgMaskMOG2, 30, 255, cv::THRESH_BINARY);
//    
//    //update the model
//    if (_testCount == 101)
//    {
//        //count numer of true elements in _fgMaskMOG2
//        onesCountPrevious = onesCount;
//        onesCount = cv::countNonZero(_fgMaskMOG2);
//        NSLog(@"onesCount: %i", onesCount);
//        
//        if (onesCount - onesCountPrevious > 5000)
//        {
//            NSLog(@"onesCountPrevious: %i", onesCountPrevious);
//            
//            //prints the current time
//            NSDate * now = [NSDate date];
//            NSDateFormatter *outputFormatter = [[NSDateFormatter alloc] init];
//            [outputFormatter setDateFormat:@"HH:mm:ss"];
//            NSString *newDateString = [outputFormatter stringFromDate:now];
//            NSLog(@"Timestamp: %@", newDateString);
//            _testCount = 0;
//        }
//        
//        //image = cv::Scalar::all(0);
//        //image = _fgMaskMOG2;
//        
//        /* Snippet that can find non-zero locations
//         cv::Mat binaryImage; // input, binary image
//         vector<Point> locations;   // output, locations of non-zero pixels
//         cv::findNonZero(binaryImage, locations);
//         // access pixel coordinates
//         Point pnt = locations[i];
//         */
//        _testCount = 0;
//        
//    }
//    else
//    {
//        image = _fgMaskMOG2;
//        //cv::Mat backgroundImage;
//        //_pMOG2->getBackgroundImage(backgroundImage);
//        //image = backgroundImage;
//        
//        //get the frame number and write it on the current frame
//        rectangle(image, cv::Point(10, 2), cv::Point(100,20),
//                  cv::Scalar(255,255,255), -1);
//        putText(image, "test", cv::Point(15, 15),
//                cv::FONT_HERSHEY_SIMPLEX, 0.5 , cv::Scalar(0,0,0));
//        _testCount++;
//    }
//    
//    //GaussianBlur(_fgMask, _fgMask, cv::Size(7, 7), 2.5, 2.5);
//    //threshold(_fgMask, _fgMask, 10, 255, cv::THRESH_BINARY);
//    
//    //image = cv::Scalar::all(0);
//    
//}
//
///*
// train the MoG background using an inistal history
// when a large increase in number of white pixels in foreground mask occurs
// record this as a suspicious event
// save camera picture for review
// save timestamp of event
// stretch: create bounding box of largest blobs and save as subimage
// 
// 
// */
//
//
///*
// - (void)processImage:(cv::Mat&)image
// {
// // Do some OpenCV stuff with the image
// //cv::Mat image_copy;
// //cvtColor(image, image_copy, CV_BGRA2BGR);
// 
// 
// cvtColor( image, image, cv::COLOR_BGR2GRAY );
// std::vector<std::vector<cv::Point> > contours;
// std::vector<cv::Vec4i> hierarchy;
// 
// // Canny edge detector ...
// Canny(image, image, 100, 300); // Adjust thresholds
// threshold(image, image, 100, 255, cv::THRESH_BINARY_INV);
// // find contours
// findContours(image, contours, hierarchy, cv::RETR_CCOMP, cv::CHAIN_APPROX_SIMPLE,     cv::Point(0,0));
// 
// _pMOG2 = cv::createBackgroundSubtractorMOG2();
// //_pMOG2->apply(image, _fgMaskMOG2);
// 
// int idx = 0;
// for (; idx >= 0; idx = hierarchy[idx][0] )
// {
// drawContours( image, contours, idx, 255, 1, 4, hierarchy );
// }
// //bitwise_not(image, image);
// std::string test = "hello colin";
// cv::putText(image, test, cv::Point(15, 15), CV_FONT_HERSHEY_SIMPLEX, 0.5 , cv::Scalar(0,0,0));
// 
// // invert image
// //bitwise_not(image_copy, image_copy);
// //cvtColor(image_copy, image, CV_BGR2BGRA);
// }*/
//#endif
//
//
//- (cv::Mat)cvMatFromUIImage:(UIImage *)image
//{
//    CGColorSpaceRef colorSpace = CGImageGetColorSpace(image.CGImage);
//    CGFloat cols = image.size.width;
//    CGFloat rows = image.size.height;
//    
//    cv::Mat cvMat(rows, cols, CV_8UC4); // 8 bits per component, 4 channels (color channels + alpha)
//    
//    CGContextRef contextRef = CGBitmapContextCreate(cvMat.data,                 // Pointer to  data
//                                                    cols,                       // Width of bitmap
//                                                    rows,                       // Height of bitmap
//                                                    8,                          // Bits per component
//                                                    cvMat.step[0],              // Bytes per row
//                                                    colorSpace,                 // Colorspace
//                                                    kCGImageAlphaNoneSkipLast |
//                                                    kCGBitmapByteOrderDefault); // Bitmap info flags
//    
//    CGContextDrawImage(contextRef, CGRectMake(0, 0, cols, rows), image.CGImage);
//    CGContextRelease(contextRef);
//    
//    return cvMat;
//}
//
//- (cv::Mat)cvMatGrayFromUIImage:(UIImage *)image
//{
//    CGColorSpaceRef colorSpace = CGImageGetColorSpace(image.CGImage);
//    CGFloat cols = image.size.width;
//    CGFloat rows = image.size.height;
//    
//    cv::Mat cvMat(rows, cols, CV_8UC1); // 8 bits per component, 1 channels
//    
//    CGContextRef contextRef = CGBitmapContextCreate(cvMat.data,                 // Pointer to data
//                                                    cols,                       // Width of bitmap
//                                                    rows,                       // Height of bitmap
//                                                    8,                          // Bits per component
//                                                    cvMat.step[0],              // Bytes per row
//                                                    colorSpace,                 // Colorspace
//                                                    kCGImageAlphaNoneSkipLast |
//                                                    kCGBitmapByteOrderDefault); // Bitmap info flags
//    
//    CGContextDrawImage(contextRef, CGRectMake(0, 0, cols, rows), image.CGImage);
//    CGContextRelease(contextRef);
//    
//    return cvMat;
//}
//
//
//- (IBAction)startTapped:(UIButton *)sender {
//    [_videoCamera start];
//    
//    //Show the untouched camera input in videoView
//    AVCaptureSession *captureSession = _videoCamera.captureSession;
//    
//    AVCaptureVideoPreviewLayer *previewLayer = [AVCaptureVideoPreviewLayer layerWithSession:captureSession];
//    //_videoCamera.defaultAVCaptureSessionPreset = AVCaptureSessionPreset1280x720;
//    [previewLayer.connection setVideoOrientation:AVCaptureVideoOrientationLandscapeLeft];
//    
//    UIView *aView = self.VideoVIew;
//    previewLayer.frame = aView.bounds; // Assume you want the preview layer to fill the view.
//    [aView.layer addSublayer:previewLayer];
//    
//}
//
//
//@end
